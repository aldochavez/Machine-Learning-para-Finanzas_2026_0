# -*- coding: utf-8 -*-
"""Modelo de Clasificación.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z1Y--TbEv8SO-mCNfTPQuie0D_wSRvii
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

# Cargar la base de datos
df = pd.read_excel("lending_dataset.xlsx")

# Visualización de la estructura inicial del DataFrame
df.head()

# Separación de variables según su tipo de dato

# Seleccionar columnas numéricas (enteros y flotantes)
numeric_cols = df.select_dtypes(include=['number']).columns.tolist()

# Seleccionar columnas categóricas (objetos/texto)
categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()

# Mostrar listas de columnas clasificadas
numeric_cols, categorical_cols

"""## 1. Analisis estadistico inicial

### 1.1 Numéricas
"""

# 1. Estadísticos base (mean, std, min, max, etc.)
# Se transpone (.T) para facilitar la lectura de múltiples variables
numeric_stats = df[numeric_cols].describe().T
numeric_stats

# 2. Identificación de colas y outliers (Cuantiles específicos)
# Lista de percentiles solicitada: 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%
percentiles = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]
numeric_quantiles = df[numeric_cols].quantile(percentiles).T
numeric_quantiles

# 3. Porcentaje de valores faltantes (% Missing)
# Calcula el porcentaje de nulos por columna numérica
missing_numeric = df[numeric_cols].isna().mean() * 100
missing_numeric = missing_numeric.to_frame(name='% Missing').sort_values(by='% Missing', ascending=False)
missing_numeric

# 4. Top-5 valores más frecuentes (Dominancia)
# Itera sobre las columnas numéricas para mostrar el conteo y porcentaje de los 5 valores más comunes
for col in numeric_cols:
    print(f"\n--- Top 5 valores en: {col} ---")
    top5 = pd.concat([df[col].value_counts().head(5),
                      df[col].value_counts(normalize=True).head(5) * 100],
                     axis=1, keys=['Count', '%'])
    display(top5)

"""### 1.2 Variables Categoricas"""

# 1. Cardinalidad (Cantidad de categorías únicas)
# Calcula valores únicos y el porcentaje que representa cada categoría
cardinality = df[categorical_cols].nunique().to_frame(name='N_Unique')
cardinality['% Unique (vs Total Rows)'] = (cardinality['N_Unique'] / len(df)) * 100
cardinality.sort_values(by='N_Unique', ascending=False)

# 2. Top-10 valores más frecuentes (Dominancia / Long Tail)
# Muestra los 10 valores más comunes incluyendo nulos (dropna=False) con su conteo y porcentaje
for col in categorical_cols:
    print(f"\n--- Top 10 valores en: {col} ---")
    top10 = pd.concat([df[col].value_counts(dropna=False).head(10),
                       df[col].value_counts(dropna=False, normalize=True).head(10) * 100],
                      axis=1, keys=['Count', '%'])
    display(top10)

"""## 2. Analisis de Calidad

### Analisis de Dtypes
"""

# Información general de dimensiones
print(f"Observaciones: {df.shape[0]}")
print(f"Variables: {df.shape[1]}")

# ---------------------------------------------------------
# 1. Tabla para Variables Numéricas
# ---------------------------------------------------------
print("\n--- Variables Numéricas (Tipos de Dato) ---")
numeric_dtypes_summary = (
    df[numeric_cols].dtypes
    .to_frame(name="Tipo de dato")
    .reset_index()
    .rename(columns={"index": "Variable"})
)

display(
    numeric_dtypes_summary
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
)

# ---------------------------------------------------------
# 2. Tabla para Variables Categóricas
# ---------------------------------------------------------
print("\n--- Variables Categóricas (Tipos de Dato) ---")
categorical_dtypes_summary = (
    df[categorical_cols].dtypes
    .to_frame(name="Tipo de dato")
    .reset_index()
    .rename(columns={"index": "Variable"})
)

display(
    categorical_dtypes_summary
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
)

"""### 2.1 Numericas"""

# 2. Missing Real (Valores faltantes)
# Ranking de porcentaje de nulos en variables numéricas
missing_real = df[numeric_cols].isna().mean().sort_values(ascending=False) * 100
missing_real.to_frame(name='% Missing Real')

# 3. Outliers (Atípicos)
# Tabla comparativa: Min, P1, Mediana, P99, Max para detectar rangos extremos
outliers_summary = df[numeric_cols].quantile([0.01, 0.5, 0.99]).T
outliers_summary['min'] = df[numeric_cols].min()
outliers_summary['max'] = df[numeric_cols].max()
# Reordenamos columnas para mejor lectura: Min -> P1 -> P50 -> P99 -> Max
outliers_summary = outliers_summary[['min', 0.01, 0.5, 0.99, 'max']]
outliers_summary

# 4. Duplicados
# Conteo de filas totalmente duplicadas en el dataset
duplicates_count = df.duplicated().sum()
print(f"Cantidad de filas duplicadas: {duplicates_count}")

"""### 2.2 Categoricas"""

# 1. Missing Standard y 2. Missing Codificado
# Analizamos nulos reales y buscamos marcadores de nulos como "?", cadenas vacías o espacios
encoded_missing_check = []

for col in categorical_cols:
    # Missing real (NaN)
    missing_pct = df[col].isna().mean() * 100

    # Missing codificado (convertimos a str para buscar patrones)
    series_str = df[col].astype(str).str.strip()
    count_qmark = (series_str == '?').sum()
    count_empty = (series_str == '').sum()

    encoded_missing_check.append({
        'Variable': col,
        '% Missing (NaN)': missing_pct,
        'Conteo "?"': count_qmark,
        'Conteo Vacio ""': count_empty
    })

pd.DataFrame(encoded_missing_check)

# 3. Categorías con baja frecuencia y 4. Valores inesperados
# Mostramos la distribución completa para detectar categorías raras (low frequency) o typos
# Se usa dropna=False para ver nulos como una categoría más
for col in categorical_cols:
    print(f"\n--- Distribución de valores: {col} ---")
    val_counts = df[col].value_counts(dropna=False).to_frame(name='Count')
    val_counts['% Freq'] = df[col].value_counts(dropna=False, normalize=True) * 100
    display(val_counts)

"""## 3. Preprocesamiento

### 3.1 Tratamiento de Valores Nulos en Variables Numéricas

**Justificación:**
Se ha identificado que las variables `pub_rec_bank`, `revol_util`, `dti`, `delinq_2yrs` y `annual_inc` presentan un porcentaje de valores faltantes muy bajo (marginal). Dado que la pérdida de datos es mínima, optamos por eliminar estas observaciones (estrategia *listwise deletion*) para preservar la distribución original de los datos y evitar sesgos introducidos por técnicas de imputación sintética (como la media o mediana) en estas variables críticas.
"""

# Lista de variables numéricas con bajo porcentaje de missing values
cols_num_missings = ['pub_rec_bankruptcies', 'revol_util', 'dti', 'delinq_2yrs', 'annual_inc']

# Eliminación de filas con nulos en las columnas seleccionadas
df.dropna(subset=cols_num_missings, inplace=True)

"""### 3.2 Recategorización

**Justificación:**
*   **`emp_length`**: La variable presenta múltiples categorías intermedias (1 año, 2 años, etc.). Agruparlas en tres segmentos claros (`10+ years`, `< 1 year` y `1-10 years`) reduce la cardinalidad, facilita la interpretación del modelo y evita el sobreajuste en categorías con menor frecuencia.
*   **`home_ownership`**: La categoría `NONE` tiene una frecuencia residual (incluso de una sola observación). Para garantizar la estabilidad del modelo y eliminar ruido, se fusiona esta categoría dentro de `OTHER`.
"""

# 1. Agrupación de emp_length en 3 grandes categorías
# Definimos las categorías que queremos mantener intactas
keep_cats = ['10+ years', '< 1 year']

# Aplicamos la lógica: Si no es nulo y no está en la lista de mantener, se convierte a '1-10 years'
mask_change = (~df['emp_length'].isin(keep_cats)) & (df['emp_length'].notna())
df.loc[mask_change, 'emp_length'] = '1-10 years'

# 2. Corrección en home_ownership
# Fusión de la categoría minoritaria 'NONE' en 'OTHER'
df['home_ownership'] = df['home_ownership'].replace('NONE', 'OTHER')

"""### 3.3 Limpieza de Categóricas y Ruido en `addr_state`

**Justificación:**
*   **Missings Categóricos**: Al igual que con las numéricas, el volumen de nulos en las variables categóricas restantes es bajo, por lo que se procede a su eliminación para trabajar con registros completos.
*   **Ruido en `addr_state`**: Se han detectado valores anómalos (typos como "951xx") que no corresponden a códigos de estado válidos. Se aplica un filtro de longitud (estándar de 2 caracteres) para asegurar la calidad y consistencia de la variable geográfica.
"""

# 1. Eliminación de nulos en variables categóricas
# Usamos la lista de columnas categóricas definida al inicio
df.dropna(subset=categorical_cols, inplace=True)

# 2. Limpieza de valores extraños en addr_state
# Filtramos para mantener solo aquellos registros donde el estado tiene longitud 2 (Ej: 'CA', 'TX')
# Esto elimina automáticamente errores de formato
df = df[df['addr_state'].str.len() == 2]

# Reinicio del índice tras las eliminaciones para mantener orden secuencial
df.reset_index(drop=True, inplace=True)

"""### 3.4 Selección de Variables (Eliminación de Irrelevantes)

**Justificación:**
Se procede a eliminar variables que no aportan valor predictivo o son inmanejables para el modelo:
*   **`emp_title` (Alta Cardinalidad):** Posee más de 15,000 categorías únicas (texto libre). Su inclusión generaría excesivo ruido y dimensionalidad sin un procesamiento de NLP complejo.
*   **`pymnt_plan` (Varianza Cero):** Contiene un único valor para todas las observaciones, por lo que no aporta capacidad de discriminación.
*   **`application_type` (Cuasi-constante):** El 99.9% de los datos pertenece a la categoría "Individual", con casos residuales (ruido/typos) que no permiten un análisis estadístico válido.
"""

# Lista de variables a descartar por falta de varianza o exceso de cardinalidad
vars_to_drop = ['emp_title', 'pymnt_plan', 'application_type']

# Eliminamos las columnas del DataFrame (errors='ignore' evita fallos si se corre la celda dos veces)
df.drop(columns=vars_to_drop, inplace=True, errors='ignore')

# ---  Actualización de listas de metadatos ---
# Debemos actualizar las listas de columnas numéricas y categóricas
# para que los bucles de gráficos posteriores (EDA) no fallen buscando variables borradas.
numeric_cols = [col for col in numeric_cols if col in df.columns]
categorical_cols = [col for col in categorical_cols if col in df.columns]

"""## 4. Analisis exploratorio

### 4.1 Histogramas
"""

# Configuración de estilo
sns.set_style("whitegrid")

# Definimos el tamaño del grid dinámicamente según la cantidad de variables numéricas
num_vars = len(numeric_cols)
cols = 3
rows = math.ceil(num_vars / cols)

# Creación de la figura
plt.figure(figsize=(15, 4 * rows))

for i, var in enumerate(numeric_cols):
    plt.subplot(rows, cols, i + 1)
    # Histograma con curva de densidad (KDE) para ver la forma de la distribución
    sns.histplot(df[var], kde=True, bins=30)
    plt.title(f'Distribución: {var}')
    plt.xlabel(var)

plt.tight_layout()
plt.show()

"""### 4.2 Boxplots"""

# Creación de la figura para Boxplots
plt.figure(figsize=(15, 4 * rows))

for i, var in enumerate(numeric_cols):
    plt.subplot(rows, cols, i + 1)
    # Boxplot para visualizar rango intercuartílico y valores atípicos (puntos fuera de los bigotes)
    sns.boxplot(x=df[var], color='skyblue')
    plt.title(f'Boxplot: {var}')
    plt.xlabel(var)

plt.tight_layout()
plt.show()

"""### 4.3 Scatterplots"""

# Utilizamos Pairplot para generar todos los scatterplots posibles entre variables numéricas
# Esto permite detectar linealidad y clusters en una sola ejecución
# corner=True evita duplicar gráficos (espejo) y diag_kind='kde' muestra la densidad en la diagonal
sns.pairplot(df[numeric_cols], diag_kind='kde', corner=True, plot_kws={'alpha': 0.5, 's': 20})
plt.suptitle('Matriz de Scatterplots (Relaciones Bivariadas)', y=1.02)
plt.show()

"""### 4.4 Bar Plots (Frecuencia por Categoría)"""

# Configuración del grid para gráficos categóricos
# Calculamos filas necesarias según la cantidad de variables categóricas
num_cat = len(categorical_cols)
cols = 2  # 2 gráficos por fila para buena visibilidad
rows = math.ceil(num_cat / cols)

plt.figure(figsize=(15, 5 * rows))

for i, var in enumerate(categorical_cols):
    plt.subplot(rows, cols, i + 1)

    # Gráfico de conteo ordenado por frecuencia descendente
    sns.countplot(data=df, x=var, order=df[var].value_counts().index, palette='viridis')

    plt.title(f'Frecuencia: {var}')
    plt.xlabel(var)
    plt.ylabel('Conteo')
    plt.xticks(rotation=45) # Rotación para leer mejor las etiquetas

plt.tight_layout()
plt.show()

"""## 5. EDA Bivariado"""

# Creación de variable binaria para análisis de riesgo
# 1 = Charged Off (Bad), 0 = Fully Paid (Good)
df['target_def'] = df['loan_status'].apply(lambda x: 1 if x == 'Charged Off' else 0)

# Verificación de la proporción (Tasa base)
print(f"Tasa base de incumplimiento: {df['target_def'].mean():.2%}")

"""### 5.1 Numéricas vs Target"""

# Análisis Bivariado: Numéricas vs Target
# Se generan Boxplots y Tablas de Tendencia por quintiles

for col in numeric_cols:
    # 1. Gráfico: Boxplot por clase (Separación de clases)
    plt.figure(figsize=(12, 4))

    # Subplot 1: Boxplot
    plt.subplot(1, 2, 1)
    sns.boxplot(x='loan_status', y=col, data=df, palette='coolwarm')
    plt.title(f'Distribución de {col} por Loan Status')

    # 2. Tabla: Tasa por Buckets (Umbrales y no linealidad)
    # Creamos 5 grupos (quintiles). 'duplicates=drop' maneja variables con muchos valores repetidos (como 0)
    try:
        df['bucket'] = pd.qcut(df[col], q=5, duplicates='drop')
    except ValueError:
        # Fallback a 'cut' si qcut falla por distribución muy sesgada
        df['bucket'] = pd.cut(df[col], bins=5)

    bucket_analysis = df.groupby('bucket', observed=False)['target_def'].agg(['mean', 'count'])
    bucket_analysis.rename(columns={'mean': 'Tasa Default', 'count': 'N_Registros'}, inplace=True)

    # Subplot 2: Visualización de la tendencia de la tasa
    plt.subplot(1, 2, 2)
    sns.lineplot(data=bucket_analysis, x=bucket_analysis.index.astype(str), y='Tasa Default', marker='o', color='red')
    plt.xticks(rotation=45)
    plt.title(f'Tendencia de Default por quintil: {col}')

    plt.tight_layout()
    plt.show()

    # Mostrar la tabla numérica debajo de los gráficos
    print(f"--- Análisis por Buckets: {col} ---")
    display(bucket_analysis)

    # Limpieza auxiliar
    if 'bucket' in df.columns:
        df.drop(columns=['bucket'], inplace=True)

"""### 5.1 Categoricas vs Target"""

# Análisis Bivariado: Categóricas vs Target
# Se compara la tasa de incumplimiento promedio por cada categoría

for col in categorical_cols:
    # Ignoramos la variable target original para no compararla consigo misma
    if col == 'loan_status':
        continue

    plt.figure(figsize=(10, 4))

    # 1. Tabla: Cálculo de Tasa de Default y Conteo por grupo
    cat_analysis = df.groupby(col)['target_def'].agg(['mean', 'count']).sort_values(by='mean', ascending=False)
    cat_analysis.rename(columns={'mean': 'Tasa Default', 'count': 'N_Registros'}, inplace=True)

    # 2. Gráfico: Barplot de la Tasa de Default
    sns.barplot(x=cat_analysis.index, y=cat_analysis['Tasa Default'], palette='magma')
    plt.axhline(y=df['target_def'].mean(), color='r', linestyle='--', label='Tasa Promedio Global')

    plt.title(f'Tasa de Default por: {col}')
    plt.ylabel('Tasa de Incumplimiento (Riesgo)')
    plt.xticks(rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Mostrar tabla de evidencia
    print(f"--- Detalle numérico: {col} ---")
    display(cat_analysis)