# -*- coding: utf-8 -*-
"""Modelo de Regresi√≥n.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dl2W9PxzUlF5O_Fx0j1g29OtVfFP6ilE
"""

# Cargamos las librer√≠as
import pandas as pd
import numpy as np
from IPython.display import display

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import matplotlib.ticker as ticker
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor

sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (8,5)

from google.colab import drive
drive.mount('/content/drive')

# Leemos la base de datos
df = pd.read_excel("/content/drive/MyDrive/2026-0/Machine Learing para Finanzas/Trabajo 1/income_dataset.xlsx")

# Realizamos la diferenciaci√≥n de variables continuas y categ√≥ricas
num_cols = df.select_dtypes(include="number").columns.tolist()
cat_cols = df.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

# Definimos nuestra variable dependiente (Target)
TARGET = "Income"
if TARGET in num_cols:
    num_cols.remove(TARGET)

"""# **1. An√°lisis Estad√≠stico Inicial: DESCRIPTIVO**

## **1.1 Variables num√©ricas**
"""

# Estad√≠sticos descriptivos: Count, media, desvest, min, max y percentiles.
desc_num = df[num_cols].describe().T
display(desc_num)

# Cuantiles
quantiles = df[num_cols].quantile([0.01,0.05,0.10,0.25,0.5,0.75,0.9,0.95,0.99]).T
display(quantiles)

# % Missing
missing_num = df[num_cols].isna().mean().sort_values(ascending=False).to_frame("% NA")
display(missing_num)

# Top 5 valores m√°s frecuentes
for col in num_cols:
    print(f"\nTop 5 valores m√°s frecuentes de {col}")

    top5 = (
        df[col]
        .value_counts(normalize=True, dropna=False)
        .mul(100)
        .head(5)
        .to_frame(name="% sobre el total de Obs")
        .reset_index()
    )

    display(
        top5
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
        .format({"% sobre el total de Obs": "{:.2f}"})
    )

"""## **1.2 Variables Categ√≥ricas**"""

# Cardinalidad
card_cat = df[cat_cols].nunique(dropna=False).sort_values(ascending=False).to_frame(name="Cardinalidad").reset_index().rename(columns={"index": "Variables categ√≥ricas"})

display(
    card_cat
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
)

# Top 10 valores m√°s frecuentes
for col in cat_cols:
    print(f"\nTop 10 valores m√°s frecuentes de {col}")

    abs_count = df[col].value_counts(dropna=False).rename("Total")
    rel_count = (
        df[col]
        .value_counts(normalize=True, dropna=False)
        .mul(100)
        .rename("% sobre el total")
    )

    top10 = (
        pd.concat([abs_count, rel_count], axis=1)
        .head(10)
        .reset_index()
        .rename(columns={"index": col})
    )

    display(
        top10
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
        .format({"% sobre el total": "{:.2f}"})
    )

"""## **1.3 Target**"""

#Realizamos una verificaci√≥n sobre el tipo de variable y el n√∫mero de observaciones.
target_overview = pd.DataFrame({
    "Target": [TARGET],
    "Tipo de dato": [df[TARGET].dtype],
    "Observaciones totales": [df.shape[0]],
    "Observaciones nulas": [df[TARGET].isna().sum()],
    "% Nulos": [df[TARGET].isna().mean() * 100]
})


display(
    target_overview
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .format({"% Nulos": "{:.2f}"})
)

#Estad√≠sticos descriptivos del target
desc_income = (
    df[TARGET]
    .describe()
    .to_frame()
    .T
)

display(desc_income)

#Cuantiles
quantiles_income = (
    df[TARGET]
    .quantile([0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99])
    .to_frame()
    .T
)

quantiles_income.index = [TARGET]

display(quantiles_income)

#Distribuci√≥n del Income
plt.figure(figsize=(8, 5))
sns.histplot(df[TARGET], bins=30, kde=True)

plt.gca().xaxis.set_major_formatter(
    mtick.StrMethodFormatter('{x:,.0f}')
)

plt.title("Distribuci√≥n del Income")
plt.xlabel("Income")
plt.ylabel("Frecuencia")
plt.show()

#Distribuci√≥n del Income
plt.figure(figsize=(4, 6))
sns.boxplot(y=df[TARGET])

plt.gca().yaxis.set_major_formatter(
    mtick.StrMethodFormatter('{x:,.0f}')
)

plt.title("Boxplot del Income")
plt.ylabel("Income")
plt.show()

#Outliers
Q1 = df[TARGET].quantile(0.25)
Q3 = df[TARGET].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers_pct = (
    ((df[TARGET] < lower_bound) | (df[TARGET] > upper_bound))
    .mean() * 100
)

print(f"% de outliers : {outliers_pct:.2f}% de la muestra")
print(f"Valor extremo m√≠nimo: {df[TARGET].min()}")
print(f"Valor extremo m√°ximo: {df[TARGET].max()}")
print(f"Observaciones con Income igual o menor a 0: {(df[TARGET] <= 0).sum()}")

"""# **2. An√°lisis de calidad - DIAGN√ìSTICO**

### ¬øLa data es confiable, usable y coherente para un modelo de regresi√≥n de ingresos?
"""

# Dtypes incorrectos
print(f"Observaciones: {df.shape[0]}")
print(f"Variables: {df.shape[1]}")


dtype_summary = (
    df.dtypes
    .to_frame(name="Tipo de dato")
    .reset_index()
    .rename(columns={"index": "Variable"})
)

display(
    dtype_summary
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
)

"""## **2.1 Variables num√©ricas**"""

# Coerce diagn√≥stico
coerce_check = {}
for col in num_cols:
    before_na = df[col].isna().sum()
    after_na = pd.to_numeric(df[col], errors="coerce").isna().sum()
    coerce_check[col] = [before_na, after_na]

coerce_df = pd.DataFrame(coerce_check, index=["NaN Iniciales", "NaN Finales"]).T
display(coerce_df
        .style
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ]))

# Missing reales
missings = df[num_cols].isna().mean().sort_values(ascending=False)

summary = (
    missings
    .to_frame(name="Missings Reales")
    .reset_index()
    .rename(columns={"index": "Variable"})
)

display(
    summary
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({"Missings Reales": "{:.0f}"})
)

# Outliers
outliers = df[num_cols].agg(["min", lambda x: x.quantile(0.01), "median", lambda x: x.quantile(0.99), "max"]).T
outliers.columns = ["MIN", "P1", "P50", "P99", "MAX"]

outliers = (
    outliers
    .reset_index()
    .rename(columns={"index": "Variable"})
)
display(
    outliers
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({
        "MIN": "{:.2f}",
        "P1": "{:.2f}",
        "P50": "{:.2f}",
        "P99": "{:.2f}",
        "MAX": "{:.2f}"
    })
)

"""## **2.2 Variables categ√≥ricas**"""

# Missing expl√≠cito
cat_missing = (
    df[cat_cols]
    .isna()
    .agg(["sum", "mean"])
    .T
    .rename(columns={
        "sum": "Nulos",
        "mean": "% Nulos"
    })
)

cat_missing["% Nulos"] *= 100

cat_missing = (
    cat_missing
    .reset_index()
    .rename(columns={"index": "Variable"})
)

display(
    cat_missing
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({"% Nulos": "{:.2f}", "Nulos":"{:.0f}"})
)

#Missing Codificado
# Definir tokens de missing codificado
missing_tokens = [
    "unknown", "none", "missing", "na", "n/a", "?", "other", ""
]

# Inicializar DataFrame base
encoded_missing_df = pd.DataFrame(
    0,
    index=cat_cols,
    columns=missing_tokens
)

# Poblar la tabla
for col in cat_cols:
    col_values = (
        df[col]
        .astype(str)
        .str.strip()
        .str.lower()
    )

    for token in missing_tokens:
        encoded_missing_df.loc[col, token] = (col_values == token).sum()

# Formato final
encoded_missing_df = (
    encoded_missing_df
    .reset_index()
    .rename(columns={"index": "Variable"})
)

display(
    encoded_missing_df
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
)

# Categor√≠as con baja frecuencia
low_freq_summary = []

for col in cat_cols:
    freq = df[col].value_counts(normalize=True, dropna=False)

    low_freq = freq[freq < 0.01]

    low_freq_summary.append({
        "Variable": col,
        "N¬∞ categor√≠as": freq.shape[0],
        "Categor√≠as con frecuencia <1%": low_freq.shape[0],
        "% Obs en categor√≠as <1%": low_freq.sum() * 100
    })

low_freq_df = pd.DataFrame(low_freq_summary)

display(
    low_freq_df
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({"% Obs en categor√≠as <1%": "{:.2f}"})
)

#Valores inesperados consolidado

unexpected_table = []

for col in cat_cols:
    values = (df[col].astype(str).str.strip().str.lower().unique())

    unexpected_table.append({
        "Variable": col,
        "N¬∞ categor√≠as √∫nicas": len(values),
        "Categor√≠as observadas": ", ".join(sorted(values))
    })

unexpected_df = pd.DataFrame(unexpected_table)

display(
    unexpected_df
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
)

#Valores inesperados con frecuencia
for col in cat_cols:
    values_df = (
        df[col]
        .astype(str)
        .str.strip()
        .str.lower()
        .value_counts(dropna=False)
        .to_frame(name="Frecuencia")
        .reset_index()
        .rename(columns={"index": "Categor√≠a"})
    )

    print(f"\nValores observados en {col}")
    display(
        values_df
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
    )

"""# **3. Preprocesamiento**

Preparamos los datos para el modelo de regresi√≥n de ingresos, garantizando:

*   Consistencia
*   Manejo adecuado de valores missings
*   Tratamiento adecuado de variables categ√≥ricas
*   Reproductibilidad

## **3.1 Variables num√©ricas**

### **3.1.1 Imputaci√≥n de valores**

*   De acuerdo al diagn√≥stico, el dataset actual no cuenta con valores nulos (NaN).
*   Sin embargo, la imputaci√≥n se realiza para garantizar que el pipeline se encuentre matem√°ticamente bien definido para cualquier entrada v√°lida.
*   A futuro, si en el dataset aparecen observaciones con valores nulos, el pipeline ya se encuentra preparado para tratarlas. Esto permite que el modelo sea m√°s robusto y reutilizable.

Esto permite que el modelo est√© definido para cualquier observaci√≥n que respete el esquema de datos, incluso si esa observaci√≥n tiene valores nulos.
"""

#Generamos una copia de nuestro df para tener la versi√≥n preprocesada

df_num_processed = df.copy()

#Imputamos los valores nulos sin escalado.
num_imputer = SimpleImputer(strategy="median")

df_num_processed[num_cols] = num_imputer.fit_transform(
    df_num_processed[num_cols])

"""### **3.1.2 Aplicaci√≥n de LOG**"""

#Testeamos para Work_Experience
df_log_test1 = df_num_processed.copy()

log_candidates = ["Work_Experience"]

for col in log_candidates:
    df_log_test1[f"log_{col}"] = np.log1p(df_log_test1[col])

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

sns.histplot(df[col], bins=20, ax=axes[0])
axes[0].set_title(f"{col} - Original")

sns.histplot(df_log_test1[f"log_{col}"], bins=20, ax=axes[1])
axes[1].set_title(f"{col} - Log")

plt.tight_layout()
plt.show()

#Testeamos para Age
df_log_test2 = df_num_processed.copy()

log_candidates = ["Age"]

for col in log_candidates:
    df_log_test2[f"log_{col}"] = np.log1p(df_log_test2[col])

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

sns.histplot(df[col], bins=20, ax=axes[0])
axes[0].set_title(f"{col} - Original")

sns.histplot(df_log_test2[f"log_{col}"], bins=20, ax=axes[1])
axes[1].set_title(f"{col} - Log")

plt.tight_layout()
plt.show()

#Testeamos para Number_of_Dependents
df_log_test3 = df_num_processed.copy()

log_candidates = ["Number_of_Dependents"]

for col in log_candidates:
    df_log_test3[f"log_{col}"] = np.log1p(df_log_test3[col])

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

sns.histplot(df[col], bins=20, ax=axes[0])
axes[0].set_title(f"{col} - Original")

sns.histplot(df_log_test3[f"log_{col}"], bins=20, ax=axes[1])
axes[1].set_title(f"{col} - Log")

plt.tight_layout()
plt.show()

#Testeamos para Household_Size
df_log_test4 = df_num_processed.copy()

log_candidates = ["Household_Size"]

for col in log_candidates:
    df_log_test4[f"log_{col}"] = np.log1p(df_log_test4[col])

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

sns.histplot(df[col], bins=20, ax=axes[0])
axes[0].set_title(f"{col} - Original")

sns.histplot(df_log_test4[f"log_{col}"], bins=20, ax=axes[1])
axes[1].set_title(f"{col} - Log")

plt.tight_layout()
plt.show()

"""Se evalu√≥ la implementaci√≥n de transformaciones logar√≠tmicas sobre las variables num√©ricas. No obstante, el an√°lisis demostr√≥ que dichas variables presentan de por si rangos acotados, una naturaleza discreta y ausencia de colas extremas. Adem√°s, en la mayor√≠a de los casos, la transformaci√≥n logar√≠tmica introdujo mayor asimetr√≠a sin mejorar la estructura distributiva con respecto a la distribuci√≥n original.

Por ello, se decidi√≥ no aplicar transformaciones logar√≠tmicas sobre las variables.

## **3.2 Variables categ√≥ricas**

### **3.2.1 Limpieza de Strip**
"""

#Limpieza de los string y estandarizaci√≥n para eliminar espacios ocultos y asegurar consistencia.
df_cat_clean = df_num_processed.copy()

for col in cat_cols:
    df_cat_clean[col] = (
        df_cat_clean[col]
        .astype(str)
        .str.strip()
        .str.lower()
    )

"""### **3.2.2 Imputamos Missings**

*   De acuerdo al diagn√≥stico, el dataset actual no cuenta con valores nulos (NaN) ni nulos codificados.
*   Sin embargo, la imputaci√≥n se realiza para garantizar que el pipeline se encuentre matem√°ticamente bien definido para cualquier entrada v√°lida.
*   A futuro, si en el dataset aparecen observaciones con valores nulos, el pipeline ya se encuentra preparado para tratarlas. Esto permite que el modelo sea m√°s robusto y reutilizable.

Esto permite que el modelo est√© definido para cualquier observaci√≥n que respete el esquema de datos, incluso si esa observaci√≥n tiene valores nulos y nulos codificados.
"""

#Imputamos missing codificados
missing_tokens = [
    "unknown", "none", "missing", "na", "n/a", "?", "other", ""
]

df_cat_clean[cat_cols] = df_cat_clean[cat_cols].replace(
    missing_tokens,
    np.nan
)

#Imputamos missings
cat_imputer = SimpleImputer(strategy="most_frequent")

df_cat_clean[cat_cols] = cat_imputer.fit_transform(
    df_cat_clean[cat_cols]
)

"""## **3.3 Target**"""

#Aplicamos una transformaci√≥n log√≠stica
df_processed = df_cat_clean.copy()
df_processed["log_Income"] = np.log1p(df_processed["Income"])

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

sns.histplot(df_processed["Income"], bins=30, ax=axes[0])
axes[0].set_title("Income original")

sns.histplot(df_processed["log_Income"], bins=30, ax=axes[1])
axes[1].set_title("Log Income")

plt.tight_layout()
plt.show()

"""La variable Income presenta una distribuci√≥n asim√©trica con una cola derecha extensa. Por ello, ase aplic√≥ la transformaci√≥n logar√≠tmica a modo de reducir la asimetr√≠a y la influencia de valores extremos. Estoy permite que la variable sea m√°s adecuada para el modelado de la regresi√≥n.

# **4. An√°lisis Exploratorio EDA Univariado: An√°lisis Ex-Antes y Ex-post**
"""

df_raw = df.copy()          # Dataset Ex Ante
df_prep = df_processed.copy()  # Dataset Ex Post

"""## **4.1 Variables num√©ricas**

### **4.1.1 Estad√≠sticos Descriptivos**
"""

# ===============================
# Estad√≠sticos descriptivos
# Variables num√©ricas
# Ex ante vs Ex post
# ===============================

# Ex ante (df_raw)
desc_num_raw = (
    df_raw[num_cols]
    .describe()
    .T
    .reset_index()
    .rename(columns={"index": "Variable"})
)

desc_num_raw["Etapa"] = "Ex ante"

# Ex post (df_prep)
desc_num_prep = (
    df_prep[num_cols]
    .describe()
    .T
    .reset_index()
    .rename(columns={"index": "Variable"})
)

desc_num_prep["Etapa"] = "Ex post"

# Mostrar tablas por separado
print("\nEstad√≠sticos descriptivos ‚Äì Ex ante")
display(
    desc_num_raw
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
)

print("\nEstad√≠sticos descriptivos ‚Äì Ex post")
display(
    desc_num_prep
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
)

"""### **4.1.2 Cuantiles**"""

quantile_levels = [0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]
quantile_labels = [f"{int(q*100)}%" for q in quantile_levels]

# Ex ante
quantiles_raw = (
    df_raw[num_cols]
    .quantile(quantile_levels)
    .T
    .reset_index()
)

quantiles_raw.columns = ["Variable"] + quantile_labels
quantiles_raw["Etapa"] = "Ex ante"

# Ex post
quantiles_prep = (
    df_prep[num_cols]
    .quantile(quantile_levels)
    .T
    .reset_index()
)

quantiles_prep.columns = ["Variable"] + quantile_labels
quantiles_prep["Etapa"] = "Ex post"

# Display con formato correcto
print("\nCuantiles ‚Äì Ex ante")
display(
    quantiles_raw
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({col: "{:.2f}" for col in quantile_labels})
)

print("\nCuantiles ‚Äì Ex post")
display(
    quantiles_prep
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({col: "{:.2f}" for col in quantile_labels})
)

"""### **4.1.3 Missings**"""

# Ex ante
missings_raw = (
    df_raw[num_cols]
    .isna()
    .mean()
    .mul(100)
    .to_frame(name="% Missing")
    .reset_index()
    .rename(columns={"index": "Variable"})
)

missings_raw["Etapa"] = "Ex ante"

# Redondeo real
missings_raw["% Missing"] = missings_raw["% Missing"].round(2)

# Ex post
missings_prep = (
    df_prep[num_cols]
    .isna()
    .mean()
    .mul(100)
    .to_frame(name="% Missing")
    .reset_index()
    .rename(columns={"index": "Variable"})
)

missings_prep["Etapa"] = "Ex post"

# Redondeo real
missings_prep["% Missing"] = missings_prep["% Missing"].round(2)

# Display
print("\n% Missings ‚Äì Ex ante")
display(
    missings_raw
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({"% Missing": "{:.2f}"})
)

print("\n% de valores faltantes ‚Äì Ex post")
display(
    missings_prep
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({"% Missing": "{:.2f}"})
)

"""### **4.1.4 Top 5 valores m√°s frecuentes**"""

for col in num_cols:
    print(f"\nTop 5 valores m√°s frecuentes ‚Äì {col}")

    # Ex ante
    top5_raw = (
        df_raw[col]
        .value_counts()
        .head(5)
        .to_frame("Ex ante")
    )

    # Ex post
    top5_prep = (
        df_prep[col]
        .value_counts()
        .head(5)
        .to_frame("Ex post")
    )

    # Consolidamos
    top5 = (
        top5_raw
        .join(top5_prep, how="outer")
        .fillna(0)
        .astype(int)
    )

    top5.index = top5.index.astype(int)

    display(
        top5
        .reset_index()
        .rename(columns={"index": col})
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "th", "props": [("text-align", "center")]}
        ])
    )

"""### **4.1.5 Histograma**"""

for col in num_cols:
    fig, ax = plt.subplots(1, 2, figsize=(12,4))

    sns.histplot(df_raw[col], bins=30, ax=ax[0])
    sns.histplot(df_prep[col], bins=30, ax=ax[1])

    ax[0].set_title(f"{col} ‚Äì Ex ante")
    ax[1].set_title(f"{col} ‚Äì Ex post")

    plt.show()

"""### **4.1.6 Boxplot**"""

for col in num_cols:
    fig, ax = plt.subplots(1, 2, figsize=(12,4))

    sns.boxplot(x=df_raw[col], ax=ax[0])
    sns.boxplot(x=df_prep[col], ax=ax[1])

    ax[0].set_title(f"{col} ‚Äì Ex ante")
    ax[1].set_title(f"{col} ‚Äì Ex post")

    plt.show()

"""## **4.2 Variables categ√≥ricas**

### **4.2.1 Cardinalidad**
"""

card_raw = df_raw[cat_cols].nunique().to_frame("Ex ante")
card_prep = df_prep[cat_cols].nunique().to_frame("Ex post")

cardinality_compare = (
    card_raw
    .join(card_prep)
    .reset_index()
    .rename(columns={"index": "variable"})
)

display(
    cardinality_compare
    .style
    .hide(axis="index"))

"""### **4.2.2 Top 10 valores m√°s frecuentes**"""

for col in cat_cols:
    print(f"\nTop 10 valores m√°s frecuentes ‚Äì {col}")

    # Normalizamos strings solo para el an√°lisis comparativo
    raw_col = df_raw[col].astype(str).str.strip().str.lower()
    prep_col = df_prep[col].astype(str).str.strip().str.lower()

    # Definimos top 10 - Ex ante
    top_categories = raw_col.value_counts().head(10).index

    # Ex ante (%)
    raw_pct = (
        raw_col
        .value_counts(normalize=True)
        .mul(100)
        .reindex(top_categories)
    )

    # Ex post (%)
    prep_pct = (
        prep_col
        .value_counts(normalize=True)
        .mul(100)
        .reindex(top_categories)
    )

    # Consolidamos
    top10 = (
        pd.concat(
            [raw_pct, prep_pct],
            axis=1,
            keys=["Ex ante (%)", "Ex post (%)"]
        )
        .fillna(0)
        .round(2)
        .reset_index()
        .rename(columns={"index": col})
    )

    display(
        top10
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "th", "props": [("text-align", "center")]}
        ])
        .format({
            "Ex ante (%)": "{:.2f}",
            "Ex post (%)": "{:.2f}"
        })
    )

#Countplot top 10

for col in cat_cols:
    print(f"\nCountplot Top 10 ‚Äì {col}")

    # Normalizamos strings solo para an√°lisis
    raw_col = df_raw[col].astype(str).str.strip().str.lower()
    prep_col = df_prep[col].astype(str).str.strip().str.lower()

    # Definimos top 10 Ex ante
    top_categories = raw_col.value_counts().head(10).index

    # Dataframe consolidado
    plot_df = pd.concat([
        pd.DataFrame({
            col: raw_col[raw_col.isin(top_categories)],
            "Etapa": "Ex ante"
        }),
        pd.DataFrame({
            col: prep_col[prep_col.isin(top_categories)],
            "Etapa": "Ex post"
        })
    ])

    plt.figure(figsize=(8, 5))
    sns.countplot(
        data=plot_df,
        y=col,
        hue="Etapa",
        order=top_categories
    )

    plt.title(f"Top 10 categor√≠as ‚Äì {col}")
    plt.xlabel("Frecuencia")
    plt.ylabel(col)
    plt.legend(title="Etapa")
    plt.tight_layout()
    plt.show()

"""### **4.2.3 Tabla de frecuencias**"""

dominance = []

for col in cat_cols:
    raw_vc = df_raw[col].value_counts(normalize=True)
    prep_vc = df_prep[col].value_counts(normalize=True)

    dominance.append({
        "variable": col,
        "% top1 ex ante": raw_vc.iloc[0] * 100,
        "% top5 ex ante": raw_vc.iloc[:5].sum() * 100,
        "% top1 ex post": prep_vc.iloc[0] * 100,
        "% top5 ex post": prep_vc.iloc[:5].sum() * 100
    })

dominance_df = pd.DataFrame(dominance)
display(
    dominance_df
    .style
    .hide(axis="index")
    .format({"% top1 ex ante":"{:.2f}", "% top5 ex ante":"{:.0f}","% top1 ex post":"{:.2f}","% top5 ex post":"{:.0f}"}))

summary_rows = []

for col in cat_cols:

    # Normalizamos strings solo para an√°lisis
    raw_col = df_raw[col].astype(str).str.strip().str.lower()
    prep_col = df_prep[col].astype(str).str.strip().str.lower()

    # Distribuciones relativas
    raw_dist = raw_col.value_counts(normalize=True).mul(100)
    prep_dist = prep_col.value_counts(normalize=True).mul(100)

    summary_rows.append({
        "Variable": col,
        "% Top 1 Ex ante": raw_dist.iloc[0],
        "% Top 5 Ex ante": raw_dist.iloc[:5].sum(),
        "% Top 1 Ex post": prep_dist.iloc[0],
        "% Top 5 Ex post": prep_dist.iloc[:5].sum()
    })

top_summary_df = pd.DataFrame(summary_rows)

top_summary_df.iloc[:, 1:] = top_summary_df.iloc[:, 1:].round(2)

display(
    top_summary_df
    .style
    .hide(axis="index")
    .set_properties(**{"text-align": "center"})
    .set_table_styles([
        {"selector": "th", "props": [("text-align", "center")]}
    ])
    .format({
        "% Top 1 Ex ante": "{:.2f}",
        "% Top 5 Ex ante": "{:.2f}",
        "% Top 1 Ex post": "{:.2f}",
        "% Top 5 Ex post": "{:.2f}"
    })
)

"""Un 100% en % top 5  de todas las variables demuestra que total de la muestra est√° cubierta por por las 5 categor√≠as m√°s frecuentes, esto se confirma ya que que ninguna de las variables tiene m√°s de 5 categor√≠as.

# **5. EDA bivariado: Variables Explicativas vs Target**

## **5.1. Para Regresi√≥n**

### **Variables num√©ricas vs Target**

### **5.1.1 Boxplot por Buckets**
"""

def boxplot_target_by_numeric_buckets(df, num_col, target, q=5):
    temp = df[[num_col, target]].dropna().copy()

    if temp[num_col].nunique() < q:
        print(f"{num_col}: pocos valores √∫nicos para {q} buckets")
        return

    temp["bucket"] = pd.qcut(temp[num_col], q=q, duplicates="drop")

    plt.figure(figsize=(8, 5))
    sns.boxplot(x="bucket", y=target, data=temp)
    plt.xticks(rotation=0)
    plt.title(f"{target} por buckets de {num_col}")

    # üëá FORMATO REAL DEL EJE Y
    plt.gca().yaxis.set_major_formatter(
        mtick.StrMethodFormatter('{x:,.0f}')
    )

    plt.tight_layout()
    plt.show()

for col in num_cols:
  if col != target:
      boxplot_target_by_numeric_buckets(df, col, target)

"""### **5.1.2 Target por Buckets**"""

q = 5

for col in num_cols:
    if col == target:
        continue

    temp = df[[col, target]].dropna().copy()

    if temp[col].nunique() < q:
        print(f"{col}: pocos valores √∫nicos para {q} buckets")
        continue

    # Buckets
    temp["bucket"] = pd.qcut(temp[col], q=q, duplicates="drop")

    # groupby expl√≠cito + observed=True
    mean_target = temp.groupby("bucket", observed=True)[target].mean()
    median_target = temp.groupby("bucket", observed=True)[target].median()
    n_bucket = temp.groupby("bucket", observed=True).size()

    # Tabla final
    bucket_table = pd.concat(
        [n_bucket, mean_target, median_target],
        axis=1
    ).reset_index()

    bucket_table.columns = ["bucket", "n", "mean", "median"]

    bucket_table["mean"] = bucket_table["mean"].round(0).astype(int)
    bucket_table["median"] = bucket_table["median"].round(0).astype(int)

    print(f"\nTARGET POR BUCKETS ‚Äî {col}")
    display(bucket_table)

q = 5  # n√∫mero de buckets

for col in num_cols:
    if col == target:
        continue

    temp = df[[col, target]].dropna().copy()

    if temp[col].nunique() < q:
        print(f"{col}: pocos valores √∫nicos para {q} buckets")
        continue

    # Buckets sobre la variable explicativa
    temp["bucket"] = pd.qcut(temp[col], q=q, duplicates="drop")

    # Estad√≠sticos del target por bucket
    mean_target = temp.groupby("bucket", observed=True)[target].mean()
    median_target = temp.groupby("bucket", observed=True)[target].median()
    n_bucket = temp.groupby("bucket", observed=True).size()

    bucket_table = pd.concat(
        [n_bucket, mean_target, median_target],
        axis=1
    ).reset_index()

    bucket_table.columns = ["bucket", "n", "mean", "median"]

    bucket_table["mean"] = bucket_table["mean"].round(0).astype(int)
    bucket_table["median"] = bucket_table["median"].round(0).astype(int)

    x = bucket_table["bucket"].astype(str)

    # Gr√°fico 1: MEDIA
    plt.figure(figsize=(8, 5))
    plt.plot(x, bucket_table["mean"], marker="o")

    for i in range(len(x)):
        plt.text(i, bucket_table["mean"].iloc[i],
                 f"{bucket_table['mean'].iloc[i]}",
                 ha="center", va="bottom", fontsize=9)

    plt.title(f"Media de {target} por buckets de {col}")
    plt.xlabel("Bucket")
    plt.ylabel(f"Media de {target}")
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

    # Gr√°fico 2: MEDIANA
    plt.figure(figsize=(8, 5))
    plt.plot(x, bucket_table["median"], marker="o")

    for i in range(len(x)):
        plt.text(i, bucket_table["median"].iloc[i],
                 f"{bucket_table['median'].iloc[i]}",
                 ha="center", va="bottom", fontsize=9)

    plt.title(f"Mediana de {target} por buckets de {col}")
    plt.xlabel("Bucket")
    plt.ylabel(f"Mediana de {target}")
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()

def evaluar_tendencia_suave(valores):
    diffs = np.diff(valores)
    positivos = np.sum(diffs > 0)
    negativos = np.sum(diffs < 0)

    if positivos > negativos:
        return "Tendencia general creciente"
    elif negativos > positivos:
        return "Tendencia general decreciente"
    else:
        return "Sin tendencia clara / no monot√≥nica"

for col in num_cols:
    if col == target:
        continue

    temp = df[[col, target]].dropna().copy()

    if temp[col].nunique() < q:
        continue

    temp["bucket"] = pd.qcut(temp[col], q=q, duplicates="drop")

    bucket_table = temp.groupby("bucket", observed=True)[target] \
                       .median() \
                       .reset_index(name="median")

    bucket_table["median"] = bucket_table["median"].round(0).astype(int)

    tendencia = evaluar_tendencia_suave(bucket_table["median"].values)

    print(f"Tendencia ({col}): {tendencia}")

"""### **Variables categ√≥ricas vs Target**

### **5.2.1 Target medio por categor√≠a**
"""

for col in cat_cols:
    temp = df[[col, target]].dropna()

    summary = temp.groupby(col)[target] \
                  .agg(["mean", "median"]) \
                  .reset_index()

    summary["mean"] = summary["mean"].round(0).astype(int)
    summary["median"] = summary["median"].round(0).astype(int)

    print(f"\nTARGET POR CATEGOR√çA ‚Äî {col}")
    display(
        summary
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "th", "props": [("text-align", "center")]}
        ]))

"""### **5.2.2 Registros por categor√≠a**"""

for col in cat_cols:
    temp = df[col].dropna()

    counts = temp.value_counts().reset_index()
    counts.columns = [col, "n"]
    counts["%"] = (counts["n"] / counts["n"].sum() * 100).round(2)

    print(f"\nREGISTROS POR CATEGOR√çA ‚Äî {col}")
    display(
        counts
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "th", "props": [("text-align", "center")]}
        ])
        .format({"%":"{:.2f}"}))

"""### **5.2.3 Dispersi√≥n por categor√≠a**"""

#Tabla de dispersi√≥n
for col in cat_cols:
    temp = df[[col, target]].dropna()

    dispersion = temp.groupby(col)[target] \
                     .quantile([0.25, 0.75]) \
                     .unstack() \
                     .reset_index()

    dispersion.columns = [col, "p25", "p75"]

    dispersion["p25"] = dispersion["p25"].round(0).astype(int)
    dispersion["p75"] = dispersion["p75"].round(0).astype(int)

    print(f"\nDISPERSI√ìN DEL TARGET ‚Äî {col}")
    display(
        dispersion
        .style
        .hide(axis="index")
        .set_properties(**{"text-align": "center"})
        .set_table_styles([
            {"selector": "th", "props": [("text-align", "center")]}
        ]))

#Boxplot por categor√≠a

top_k = 10  # n√∫mero de categor√≠as m√°s frecuentes a graficar

for col in cat_cols:
    temp = df[[col, target]].dropna()

    # Seleccionar top-k categor√≠as por frecuencia
    top_categories = temp[col].value_counts().head(top_k).index
    temp_top = temp[temp[col].isin(top_categories)]

    plt.figure(figsize=(10, 5))
    ax = sns.boxplot(data=temp_top, x=col, y=target)

    # üîπ FIX CLAVE: formato correcto del eje Y
    ax.yaxis.set_major_formatter(
        ticker.StrMethodFormatter('{x:,.0f}')
    )

    plt.title(f"Distribuci√≥n de {target} por {col} (Top {top_k})")
    plt.xlabel(col)
    plt.ylabel(target)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()